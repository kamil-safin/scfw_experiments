{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from scipy.linalg import norm\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply power method to estimate L\n",
    "def estimate_lipschitz(hess_mult_vec,n):\n",
    "    Lest = 1\n",
    "    s_deactive = 0\n",
    "    dirr = np.ones(n)\n",
    "    if Lest == 1:\n",
    "        #Estimate Lipschitz Constant\n",
    "        for Liter in range(1, 16):\n",
    "            Dir=hess_mult_vec(dirr)\n",
    "            dirr = Dir / norm(Dir)\n",
    "        Hd = hess_mult_vec(dirr)\n",
    "        dHd  = dirr.dot(Hd)\n",
    "        L = dHd / (dirr.dot(dirr))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Newton direction by conjugate gradient method\n",
    "def conj_grad(Grad, Hopr, x, sc_params):\n",
    "    r=-Grad-Hopr(x)\n",
    "    x_new=x\n",
    "    k=0\n",
    "    conj_iter=sc_params['conj_grad_iter']\n",
    "    eps=sc_params['conj_grad_tol']\n",
    "    while norm(r)>eps and k<conj_iter:\n",
    "        p = r\n",
    "        Hp = Hopr(p)\n",
    "        alph = r.dot(r)/(p.dot(Hp))\n",
    "        x_new = x_new+alph*p\n",
    "        r_new=r-alph*Hp\n",
    "        bet=r_new.dot(r_new)/r.dot(r)\n",
    "        p=p+bet*p\n",
    "        r=r_new\n",
    "        k=k+1\n",
    "        \n",
    "    return x_new    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fista(func,Grad_func, prox_func, Hopr, x, sc_params):\n",
    "    y = x.copy()\n",
    "    n=len(y)\n",
    "    Lest=sc_params['Lest']\n",
    "    fista_type=sc_params['fista_type']\n",
    "    if Lest=='estimate':\n",
    "        L=estimate_lipschitz(Hopr,n)  \n",
    "    elif Lest=='backtracking':    \n",
    "        L=1\n",
    "    x_cur = y.copy()\n",
    "    f_cur=func(x_cur)\n",
    "    fista_iter=sc_params['fista_iter']\n",
    "    tol=sc_params['fista_tol']\n",
    "    t=1\n",
    "    for k in range(1, fista_iter + 1):\n",
    "        grad_y=Grad_func(y)\n",
    "        if Lest=='estimate':\n",
    "            x_tmp = y - 1 / L * grad_y\n",
    "            #x_tmps.append(x_tmp)\n",
    "            z = prox_func(x_tmp,L)\n",
    "            f_nxt=func(z)\n",
    "        elif Lest=='backtracking':\n",
    "            f_y=func(y)\n",
    "            beta=2\n",
    "            z=y\n",
    "            L=L/beta\n",
    "            diff_yz=z-y\n",
    "            f_z=f_y+grad_y.T.dot(diff_yz)+(L/2)*norm(diff_yz)**2+1\n",
    "            while f_z>f_y+grad_y.T.dot(diff_yz)+(L/2)*norm(diff_yz)**2:\n",
    "                L=L*beta\n",
    "                x_tmp = y - 1/L * Grad_func(y)\n",
    "                z = prox_func(x_tmp,L)\n",
    "                f_z = func(z)\n",
    "                diff_yz=z-y\n",
    "            f_nxt=func(z)   \n",
    "            \n",
    "        if f_nxt>f_cur and fista_type=='mfista':\n",
    "            x_nxt = x_cur\n",
    "            f_nxt = f_cur\n",
    "        else:\n",
    "            x_nxt=z\n",
    "        zdiff = z - x_cur\n",
    "        ndiff = norm(zdiff)    \n",
    "        if (ndiff < tol) and (k > 1):\n",
    "            print('Fista err = %3.3e; Subiter = %3d; subproblem converged!\\n' % (ndiff, k))\n",
    "            break\n",
    "        xdiff = x_nxt - x_cur\n",
    "        t_nxt = 0.5 * (1 + np.sqrt(1 + 4 * t**2))\n",
    "        y = x_nxt + (t - 1) / t_nxt * xdiff+t/t_nxt*zdiff\n",
    "        t  = t_nxt\n",
    "        x_cur = x_nxt\n",
    "        f_cur = f_nxt\n",
    "    return x_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =    1, stepsize = 6.882e-01, rdiff = 5.129e-01 , f = 74848\n",
      "\n",
      "Convergence achieved!\n",
      "iter =    5, stepsize = 0.000e+00, rdiff = 0.000e+00,value=32873.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def scopt(func_x,\n",
    "          grad_x,\n",
    "          hess_mult,\n",
    "          hess_mult_vec,\n",
    "          Mf,\n",
    "          nu,\n",
    "          prox_func,\n",
    "          x0,\n",
    "          sc_params,\n",
    "          eps=0.001,\n",
    "          print_every=100):\n",
    "\n",
    "    x=x0\n",
    "    n=len(x)\n",
    "    x_hist = []\n",
    "    alpha_hist = []\n",
    "    Q_hist = []\n",
    "    time_hist = []\n",
    "    grad_hist = []\n",
    "    err_hist = []\n",
    "    int_start = time.time()\n",
    "    time_hist.append(0)\n",
    "    max_iter=sc_params['iter_SC']\n",
    "    Lest=sc_params['Lest']\n",
    "    func=lambda xx: (func_x(xx))[0]\n",
    "    bPhase2=False\n",
    "    use_two_phase=sc_params['use_two_phase']\n",
    "    for i in range(1, max_iter + 1):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        Q,extra_param=func_x(x)\n",
    "        Grad = grad_x(x,extra_param)\n",
    "        Hopr=lambda s: hess_mult_vec(s,extra_param) \n",
    "        #compute local Lipschitz constant\n",
    "         \n",
    "        Newton_dir=conj_grad(Grad, Hopr, x, sc_params)\n",
    "        grad_func=lambda xx: Hopr(xx-x-Newton_dir)\n",
    "        x_nxt = fista(func,grad_func, prox_func, Hopr, x,sc_params)\n",
    "        diffx = x_nxt - x\n",
    "\n",
    "\n",
    "        lam_k=np.sqrt(hess_mult(diffx, extra_param))\n",
    "        beta_k=Mf*norm(diffx)\n",
    "        # solution value stop-criterion    \n",
    "        nrm_dx = norm(diffx)\n",
    "        rdiff = nrm_dx / max(1.0, norm(x))\n",
    "        if use_two_phase and not(bPhase2):\n",
    "            if nu==2: #conditions to go to phase 2\n",
    "                #sigma_k= #still need to add something to compute sigma\n",
    "                if lambda_k*Mf/sqrt(sigma_k)<0.12964:\n",
    "                    bPhase2=True\n",
    "            elif nu<3:\n",
    "                d_nu=1 #too complicated to implement\n",
    "                if lam_k*Mf/(sigma_k)**((3-nu)/2)<min(2*d_nu/(nu-2),1/2):\n",
    "                    bPhase2=True\n",
    "            elif nu==3: \n",
    "                if lam_k*2*Mf<1:\n",
    "                    bPhase2=True\n",
    "        if not(bPhase2):#if we are not in phase 2        \n",
    "            if beta_k==0:\n",
    "                tau_k=0\n",
    "            else:\n",
    "                if nu==2:\n",
    "                    tau_k=1/beta_k*np.log(1+beta_k)\n",
    "                elif nu==3:\n",
    "                    d_k=0.5*Mf*lam_k\n",
    "                    tau_k=1/(1+d_k)\n",
    "                elif nu<3:\n",
    "                    d_k=(nu/2-1)*(Mf*lam_k)**(nu-2)*beta_k**(3-nu)\n",
    "                    nu_param=(nu-2)/(4-nu)\n",
    "                    tau_k=(1-(1+d_k/nu_param)**(-nu_param))/d_k    \n",
    "                else:\n",
    "                    sys.exit('The value of nu is not valid')\n",
    "        else:#if we are in phase 2\n",
    "            tau_k=1\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        alpha_hist.append(tau_k)\n",
    "        x_hist.append(x)\n",
    "        Q_hist.append(Q)\n",
    "        grad_hist.append(Grad)\n",
    "        err_hist.append(rdiff)\n",
    "        time_hist.append(end - start)\n",
    "        \n",
    "        x = x + tau_k * diffx\n",
    "\n",
    "        \n",
    "        # Check the stopping criterion.\n",
    "        if (rdiff <= eps) and i > 1:\n",
    "            print('Convergence achieved!')\n",
    "            print('iter = %4d, stepsize = %3.3e, rdiff = %3.3e,value=%g\\n' % (i, tau_k, rdiff,Q))\n",
    "            x_hist.append(x)\n",
    "            Q_hist.append(Q)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        if (i % print_every == 0) or (i == 1):\n",
    "            print('iter = %4d, stepsize = %3.3e, rdiff = %3.3e , f = %g\\n' % (i, tau_k, rdiff,Q))\n",
    "\n",
    "        # if mod(iter, options.printst) ~= 0\n",
    "        #     fprintf('iter = %4d, stepsize = %3.3e, rdiff = %3.3e\\n', iter, s, rdiff);\n",
    "        # end\n",
    "\n",
    "    int_end = time.time()\n",
    "    if i >= max_iter:\n",
    "        x_hist.append(x)\n",
    "        Q_hist.append(Q)\n",
    "        print('Exceed the maximum number of iterations')\n",
    "    print(int_end - int_start)    \n",
    "    return x, alpha_hist, Q_hist, time_hist, grad_hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
