{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.linalg import norm\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha policies\n",
    "def alpha_standard(k):\n",
    "    return 2 / (k + 2)\n",
    "\n",
    "def alpha_icml(Gap, hess_mult_v,d,Mf,nu):\n",
    "    e = hess_mult_v ** 0.5\n",
    "    bet = norm(d)\n",
    "    if nu==2:\n",
    "        delta=Mf*bet\n",
    "        t=1/delta*np.log(1+Gap/(delta*e**2))\n",
    "    elif nu==3:\n",
    "        delta=1/2*Mf*e\n",
    "        t = Gap / (Gap*delta + e**2)\n",
    "    else:\n",
    "        delta=(nu-2)/2*Mf*(beta**(3-nu))*e**(nu-2)\n",
    "        if nu==4:\n",
    "            t=1/delta*(1-np.exp(-delta*Gap/(e**2)))\n",
    "        elif nu<4 and nu>2:\n",
    "            const=(4-nu)/(nu-2)\n",
    "            t=1/delta*(1-(1+(-delta*Gap*const/(e**2)))**(-1/const))\n",
    "    return min(1, t)\n",
    "\n",
    "\n",
    "def alpha_line_search(k,grad_function,delta_x,beta,accuracy):\n",
    "    lb=grad_function(0).T.dot(delta_x);\n",
    "    t_lb=0\n",
    "    ub=grad_function(beta).T.dot(delta_x);\n",
    "    t_ub=beta\n",
    "    t=t_ub\n",
    "    while t_ub<1 and ub<0:\n",
    "        t_ub=1-(1-t_ub)/2\n",
    "        ub=grad_function(t_ub).T.dot(delta_x);\n",
    "    while t_ub-t_lb>accuracy:\n",
    "        t=(t_lb+t_ub)/2\n",
    "        val=grad_function(t).T.dot(delta_x);\n",
    "        if val>0:\n",
    "            t_ub=t\n",
    "        else:\n",
    "            t_lb=t \n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frank_wolfe(fun_x,\n",
    "                grad_x,\n",
    "                grad_beta,\n",
    "                hess_mult_x,\n",
    "                extra_fun,\n",
    "                Mf,\n",
    "                nu,\n",
    "                linear_oracle,\n",
    "                x_0,\n",
    "                FW_params,\n",
    "                alpha_policy='standard',\n",
    "                eps=0.001,\n",
    "                print_every=100,\n",
    "                debug_info=False):\n",
    "    \"\"\"\n",
    "        fun_x -- function, outputs function value and extra parameters\n",
    "        grad_x -- grad value\n",
    "        grad_beta -- grad velue for convex combination\n",
    "        hess_mult_x -- Hessian multiplies by x on both sides\n",
    "        extra_fun -- extra function used to transfer to other methods\n",
    "        Mf --parameter for self concordence\n",
    "        x_0 -- starting point (n)\n",
    "        alpha_policy -- name of alpha changing policy function\n",
    "        eps -- epsilon\n",
    "        print_every -- print results every print_every steps\n",
    "    \"\"\"\n",
    "    n=len(x_0)\n",
    "    lower_bound = float(\"-inf\")\n",
    "    upper_bound = float(\"inf\")\n",
    "    \n",
    "    \n",
    "    criterion=1e10*eps\n",
    "    x = x_0\n",
    "    x_hist = []\n",
    "    alpha_hist = []\n",
    "    Gap_hist = []\n",
    "    Q_hist = []\n",
    "    time_hist = []\n",
    "    grad_hist = []\n",
    "    time_hist.append(0)\n",
    "    int_start= time.time()\n",
    "    print('********* Algorithm starts *********')\n",
    "    max_iter=FW_params['iter_FW']\n",
    "    line_search_tol=FW_params['line_search_tol']\n",
    "    for k in range(1, max_iter + 1):\n",
    "        start_time = time.time()\n",
    "        Q, extra_params = fun_x(x)\n",
    "        if min(extra_params)<-1e-10: #this is a way to know if the gradient is defined on x\n",
    "            print(\"gradiend is not defined\")\n",
    "            break\n",
    "            \n",
    "        #find optimal\n",
    "        grad = grad_x(x,extra_params)\n",
    "        s=linear_oracle(grad)\n",
    "       \n",
    "        deltax=x-s  \n",
    "        Gap = grad @ deltax\n",
    "        lower_bound=max(lower_bound,Q-Gap)\n",
    "        upper_bound=min(upper_bound,Q)\n",
    "        \n",
    "        if alpha_policy == 'standard':\n",
    "            alpha = alpha_standard(k)\n",
    "        elif alpha_policy == 'line_search':\n",
    "            extra_param_s = extra_fun(s) #this is a way to know if the gradient is defined on s\n",
    "            if min(extra_param_s)==0: #if 0 it is not defines and beta is adjusted\n",
    "                beta=0.5\n",
    "            else:\n",
    "                beta=1  \n",
    "            my_grad_beta= lambda beta:grad_beta(x,s,beta,extra_params,extra_param_s)   \n",
    "            alpha = alpha_line_search(k,my_grad_beta,-deltax,beta,line_search_tol)\n",
    "        elif alpha_policy == 'icml':\n",
    "            hess_mult = hess_mult_x(s-x, extra_params)\n",
    "            alpha = alpha_icml(Gap, hess_mult,-deltax,Mf,nu)\n",
    "                      \n",
    "        # filling history\n",
    "        x_hist.append(x)\n",
    "        alpha_hist.append(alpha)\n",
    "        Gap_hist.append(Gap)\n",
    "        Q_hist.append(Q)\n",
    "        grad_hist.append(grad)\n",
    "        time_hist.append(time.time() - start_time)\n",
    "        \n",
    "        x = x + alpha * (s - x)\n",
    "        \n",
    "        criterion = min(criterion,norm(x - x_hist[-1]) / max(1, norm(x_hist[-1])))\n",
    "        #criterion = Gap\n",
    "        #print(upper_bound)\n",
    "        #print(lower_bound)\n",
    "        #criterion=(upper_bound-lower_bound)/abs(lower_bound)\n",
    "        if criterion <= eps:\n",
    "            \n",
    "            x_hist.append(x)\n",
    "            Q_hist.append(Q)\n",
    "            Q, _ = fun_x(x)\n",
    "            print('Convergence achieved!')\n",
    "            #print(f'x = {x}')\n",
    "            #print(f'v = {v}')\n",
    "            print(f'iter = {k}, stepsize = {alpha}, crit = {criterion}, upper_bound={upper_bound}, lower_bound={lower_bound}')\n",
    "            return x, alpha_hist, Gap_hist, Q_hist, time_hist, grad_hist\n",
    "                  \n",
    "        \n",
    "        if k % print_every == 0 or k == 1:\n",
    "            if not debug_info:\n",
    "                print(f'iter = {k}, stepsize = {alpha}, criterion = {criterion}, upper_bound={upper_bound}, lower_bound={lower_bound}')\n",
    "            else:\n",
    "                print(k)\n",
    "                print(f'Q = {Q}')\n",
    "                print(f's = {np.nonzero(s)}')\n",
    "                print(f'Gap = {Gap}')\n",
    "                print(f'alpha = {alpha}')\n",
    "                print(f'criterion = {criterion}')\n",
    "                #print(f'grad = {grad}')\n",
    "                print(f'grad norm = {norm(grad)}')\n",
    "                print(f'min abs dot = {min(abs(dot_product))}')\n",
    "                #print(f'x = {x}')\n",
    "                #x_nz = x[np.nonzero(x)[0]]\n",
    "                #print(f'x non zero: {list(zip(x_nz, np.nonzero(x)[0]))}\\n')\n",
    "   \n",
    "    x_hist.append(x)\n",
    "    Q_hist.append(Q)\n",
    "    int_end= time.time()\n",
    "    print(int_end - int_start)\n",
    "    return x, alpha_hist, Gap_hist, Q_hist, time_hist, grad_hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
